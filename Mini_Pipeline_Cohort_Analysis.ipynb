{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mini-Pipeline: Cohort Analysis",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/theventurecity/data-toolkit/blob/master/Mini_Pipeline_Cohort_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "OpetXnsuOL_r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![TheVentureCity](https://theventure.city/wp-content/uploads/2017/06/Theventurecity-logoweb-1.png)\n",
        "\n",
        "# Mini-Pipeline: Cohort Analysis\n",
        "1. Extract raw event log data from a CSV\n",
        "2. Transform that data into a cohort analysis dataframe\n",
        "3. Load the transformed data into Google Sheets \n",
        "4. Visualize insights in Google Data Studio\n",
        "\n",
        "## Before you begin\n",
        "\n",
        "- This notebook is shared with read-only access. To run this notebook yourself, first click \"**Open in Playground**\" in the toolbar above. That will create a separate instance that you can run and/or save a copy of to your own Google Drive. \n",
        "\n",
        "- To run each cell, hit **Shift-Enter**, which will run the contents of the active cell and move to the next cell. This includes the markup cells (such as this one).\n",
        "\n",
        "- When you run the first block of Python code, you will get a message that says, \"**Warning: This notebook was not authored by Google.**\" Please be aware that we are **NOT** accessing your data shared with Google or reading data and credentials from other sessions. This notebook reads data from GitHub and writes to a Google Sheet that only you have access to and can control. We recommend you click the box to \"**Reset all runtimes before running**\" for extra information security."
      ]
    },
    {
      "metadata": {
        "id": "GHB2MNA2zwjo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Import relevant Python libraries"
      ]
    },
    {
      "metadata": {
        "id": "9-n5RAG2N9-Z",
        "colab_type": "code",
        "outputId": "65f6fdc4-3d20-442b-acfb-7cb981309e9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "### Pandas, Numpy, and date functions to read the data from its source\n",
        "### and manipulate it in memory\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import timedelta\n",
        "from datetime import datetime\n",
        "import math\n",
        "\n",
        "\n",
        "### Google account authentication to give the code access to your Google account\n",
        "### and gspread to enable writing to Google Sheets\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import gspread\n",
        "from gspread_dataframe import set_with_dataframe\n",
        "\n",
        "\n",
        "### The IPython.display library allows us to embed an iFrame within this \n",
        "### notebook\n",
        "from IPython.display import IFrame\n",
        "\n",
        "\n",
        "### To run this with functions from TheVentureCity's GitHub repository,\n",
        "### clone the repository to the Google Colaboratory runtime environment\n",
        "### and then import the code. This code allows us to run pre-existing functions \n",
        "### rather than having to define them inline within the notebook\n",
        "### THIS IS ONLY APPLICABLE IF YOU WANT TO ACCESS THOSE FUNCTIONS ###\n",
        "from importlib.machinery import SourceFileLoader\n",
        "!git clone https://github.com/theventurecity/data-toolkit.git /tmp/theventurecity\n",
        "!mv /tmp/theventurecity/python/tvc_transform.py tvc_transform.py\n",
        "!mv /tmp/theventurecity/python/tvc_load_colab.py tvc_load_colab.py\n",
        "!rm -r /tmp/theventurecity\n",
        "tvct = SourceFileLoader('tvc_transform', 'tvc_transform.py').load_module()\n",
        "tvcl = SourceFileLoader('tvc_load_colab', 'tvc_load_colab.py').load_module()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into '/tmp/theventurecity'...\n",
            "remote: Enumerating objects: 108, done.\u001b[K\n",
            "remote: Counting objects: 100% (108/108), done.\u001b[K\n",
            "remote: Compressing objects: 100% (88/88), done.\u001b[K\n",
            "remote: Total 108 (delta 53), reused 62 (delta 19), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (108/108), 37.93 MiB | 21.58 MiB/s, done.\n",
            "Resolving deltas: 100% (53/53), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DHPavVhwwSMQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1. Extract raw event log data from a CSV"
      ]
    },
    {
      "metadata": {
        "id": "jgo6Umpxw5I1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This example uses a data file for a Sample Company from our GitHub repository. In this step we read the data file into memory as a Pandas dataframe we name \"t.\""
      ]
    },
    {
      "metadata": {
        "id": "v3KNCE2Sb3-S",
        "colab_type": "code",
        "outputId": "70d09af2-d84c-4986-98d7-caeb3f56a674",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "cell_type": "code",
      "source": [
        "filename = 'https://raw.githubusercontent.com/theventurecity/Analytics/master/data/ServBiz_transactions.csv'\n",
        "t = pd.read_csv(filename)\n",
        "t.tail(10)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>client_id</th>\n",
              "      <th>date</th>\n",
              "      <th>value_usd</th>\n",
              "      <th>segment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>420781</th>\n",
              "      <td>27902A</td>\n",
              "      <td>2019-02-28</td>\n",
              "      <td>8.75</td>\n",
              "      <td>Enterprise</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>420782</th>\n",
              "      <td>34181A</td>\n",
              "      <td>2019-02-28</td>\n",
              "      <td>18.97</td>\n",
              "      <td>SMB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>420783</th>\n",
              "      <td>30168A</td>\n",
              "      <td>2019-02-28</td>\n",
              "      <td>17.73</td>\n",
              "      <td>SMB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>420784</th>\n",
              "      <td>30844A</td>\n",
              "      <td>2019-02-28</td>\n",
              "      <td>19.98</td>\n",
              "      <td>SMB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>420785</th>\n",
              "      <td>35815A</td>\n",
              "      <td>2019-02-28</td>\n",
              "      <td>17.98</td>\n",
              "      <td>SMB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>420786</th>\n",
              "      <td>16958A</td>\n",
              "      <td>2019-02-28</td>\n",
              "      <td>17.45</td>\n",
              "      <td>SMB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>420787</th>\n",
              "      <td>13090A</td>\n",
              "      <td>2019-02-28</td>\n",
              "      <td>13.48</td>\n",
              "      <td>SMB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>420788</th>\n",
              "      <td>19162A</td>\n",
              "      <td>2019-02-28</td>\n",
              "      <td>13.64</td>\n",
              "      <td>Enterprise</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>420789</th>\n",
              "      <td>28409A</td>\n",
              "      <td>2019-02-28</td>\n",
              "      <td>14.72</td>\n",
              "      <td>SMB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>420790</th>\n",
              "      <td>12080A</td>\n",
              "      <td>2019-02-28</td>\n",
              "      <td>18.32</td>\n",
              "      <td>SMB</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       client_id        date  value_usd     segment\n",
              "420781    27902A  2019-02-28       8.75  Enterprise\n",
              "420782    34181A  2019-02-28      18.97         SMB\n",
              "420783    30168A  2019-02-28      17.73         SMB\n",
              "420784    30844A  2019-02-28      19.98         SMB\n",
              "420785    35815A  2019-02-28      17.98         SMB\n",
              "420786    16958A  2019-02-28      17.45         SMB\n",
              "420787    13090A  2019-02-28      13.48         SMB\n",
              "420788    19162A  2019-02-28      13.64  Enterprise\n",
              "420789    28409A  2019-02-28      14.72         SMB\n",
              "420790    12080A  2019-02-28      18.32         SMB"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "aTeIz9GPwYV1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2. Transform the raw data into engagement analysis dataframes\n",
        "**Note**: For a more detailed discussion about creating the DAU and DAU Decorated dataframes, complete with inline code, visit [Create the DAU Decorated Data Set](https://colab.research.google.com/drive/12uehG2EcIqxcTazKs-pNQRTQSckllOmE)\n",
        "### 2.1 Create Daily Active Users (DAU) dataframe\n",
        "The **DAU** dataframe aggregates all activity by user and day. "
      ]
    },
    {
      "metadata": {
        "id": "OvvX_fkKOmME",
        "colab_type": "code",
        "outputId": "f91db65f-824d-4dc0-8c5f-a8541cd9781d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "cell_type": "code",
      "source": [
        "# Run the create_dau_df function and show the first ten rows of the resulting dataframe\n",
        "dau = tvct.create_dau_df(t, \n",
        "                         user_id = 'client_id', \n",
        "                         activity_date = 'date', \n",
        "                         inc_amt = 'value_usd',\n",
        "                         segment_col = 'segment'\n",
        "                        )\n",
        "dau.head(10)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>activity_date</th>\n",
              "      <th>segment</th>\n",
              "      <th>inc_amt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10000A</td>\n",
              "      <td>2015-10-14</td>\n",
              "      <td>SMB</td>\n",
              "      <td>11.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10001A</td>\n",
              "      <td>2015-10-14</td>\n",
              "      <td>SMB</td>\n",
              "      <td>13.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10001A</td>\n",
              "      <td>2015-11-02</td>\n",
              "      <td>SMB</td>\n",
              "      <td>7.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10001A</td>\n",
              "      <td>2015-11-22</td>\n",
              "      <td>SMB</td>\n",
              "      <td>18.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10001A</td>\n",
              "      <td>2017-04-04</td>\n",
              "      <td>SMB</td>\n",
              "      <td>6.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>10001A</td>\n",
              "      <td>2017-12-08</td>\n",
              "      <td>SMB</td>\n",
              "      <td>8.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>10002A</td>\n",
              "      <td>2015-10-14</td>\n",
              "      <td>Enterprise</td>\n",
              "      <td>11.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>10002A</td>\n",
              "      <td>2015-10-26</td>\n",
              "      <td>Enterprise</td>\n",
              "      <td>12.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>10002A</td>\n",
              "      <td>2015-11-16</td>\n",
              "      <td>Enterprise</td>\n",
              "      <td>12.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10002A</td>\n",
              "      <td>2015-11-23</td>\n",
              "      <td>Enterprise</td>\n",
              "      <td>12.25</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  user_id activity_date     segment  inc_amt\n",
              "0  10000A    2015-10-14         SMB    11.75\n",
              "1  10001A    2015-10-14         SMB    13.75\n",
              "2  10001A    2015-11-02         SMB     7.50\n",
              "3  10001A    2015-11-22         SMB    18.00\n",
              "4  10001A    2017-04-04         SMB     6.25\n",
              "5  10001A    2017-12-08         SMB     8.75\n",
              "6  10002A    2015-10-14  Enterprise    11.75\n",
              "7  10002A    2015-10-26  Enterprise    12.25\n",
              "8  10002A    2015-11-16  Enterprise    12.25\n",
              "9  10002A    2015-11-23  Enterprise    12.25"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "CGXjLYIjxgo5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.2 - 2.3 Calculate First Date and DAU Decorated dataframes\n",
        "The create_dau_decorated_df function calls the create_first_dt_df if no first_dt dataframe is specified"
      ]
    },
    {
      "metadata": {
        "id": "0Rtr2KqNx5Kh",
        "colab_type": "code",
        "outputId": "065d9c78-bbb2-4593-de33-cda38a747b8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        }
      },
      "cell_type": "code",
      "source": [
        "# Run the create_dau_decorated_df function and show the first ten rows of the resulting dataframe\n",
        "dau_decorated = tvct.create_dau_decorated_df(dau)\n",
        "dau_decorated.head(10)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating DAU Decorated dataframe\n",
            "Creating first_dt dataframe\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>activity_date</th>\n",
              "      <th>segment</th>\n",
              "      <th>inc_amt</th>\n",
              "      <th>first_dt</th>\n",
              "      <th>first_week</th>\n",
              "      <th>first_month</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10000A</td>\n",
              "      <td>2015-10-14</td>\n",
              "      <td>SMB</td>\n",
              "      <td>11.75</td>\n",
              "      <td>2015-10-14</td>\n",
              "      <td>2015-10-12/2015-10-18</td>\n",
              "      <td>2015-10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10001A</td>\n",
              "      <td>2015-10-14</td>\n",
              "      <td>SMB</td>\n",
              "      <td>13.75</td>\n",
              "      <td>2015-10-14</td>\n",
              "      <td>2015-10-12/2015-10-18</td>\n",
              "      <td>2015-10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10001A</td>\n",
              "      <td>2015-11-02</td>\n",
              "      <td>SMB</td>\n",
              "      <td>7.50</td>\n",
              "      <td>2015-10-14</td>\n",
              "      <td>2015-10-12/2015-10-18</td>\n",
              "      <td>2015-10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10001A</td>\n",
              "      <td>2015-11-22</td>\n",
              "      <td>SMB</td>\n",
              "      <td>18.00</td>\n",
              "      <td>2015-10-14</td>\n",
              "      <td>2015-10-12/2015-10-18</td>\n",
              "      <td>2015-10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10001A</td>\n",
              "      <td>2017-04-04</td>\n",
              "      <td>SMB</td>\n",
              "      <td>6.25</td>\n",
              "      <td>2015-10-14</td>\n",
              "      <td>2015-10-12/2015-10-18</td>\n",
              "      <td>2015-10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>10001A</td>\n",
              "      <td>2017-12-08</td>\n",
              "      <td>SMB</td>\n",
              "      <td>8.75</td>\n",
              "      <td>2015-10-14</td>\n",
              "      <td>2015-10-12/2015-10-18</td>\n",
              "      <td>2015-10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>10002A</td>\n",
              "      <td>2015-10-14</td>\n",
              "      <td>Enterprise</td>\n",
              "      <td>11.75</td>\n",
              "      <td>2015-10-14</td>\n",
              "      <td>2015-10-12/2015-10-18</td>\n",
              "      <td>2015-10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>10002A</td>\n",
              "      <td>2015-10-26</td>\n",
              "      <td>Enterprise</td>\n",
              "      <td>12.25</td>\n",
              "      <td>2015-10-14</td>\n",
              "      <td>2015-10-12/2015-10-18</td>\n",
              "      <td>2015-10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>10002A</td>\n",
              "      <td>2015-11-16</td>\n",
              "      <td>Enterprise</td>\n",
              "      <td>12.25</td>\n",
              "      <td>2015-10-14</td>\n",
              "      <td>2015-10-12/2015-10-18</td>\n",
              "      <td>2015-10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10002A</td>\n",
              "      <td>2015-11-23</td>\n",
              "      <td>Enterprise</td>\n",
              "      <td>12.25</td>\n",
              "      <td>2015-10-14</td>\n",
              "      <td>2015-10-12/2015-10-18</td>\n",
              "      <td>2015-10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  user_id activity_date     segment  inc_amt    first_dt  \\\n",
              "0  10000A    2015-10-14         SMB    11.75  2015-10-14   \n",
              "1  10001A    2015-10-14         SMB    13.75  2015-10-14   \n",
              "2  10001A    2015-11-02         SMB     7.50  2015-10-14   \n",
              "3  10001A    2015-11-22         SMB    18.00  2015-10-14   \n",
              "4  10001A    2017-04-04         SMB     6.25  2015-10-14   \n",
              "5  10001A    2017-12-08         SMB     8.75  2015-10-14   \n",
              "6  10002A    2015-10-14  Enterprise    11.75  2015-10-14   \n",
              "7  10002A    2015-10-26  Enterprise    12.25  2015-10-14   \n",
              "8  10002A    2015-11-16  Enterprise    12.25  2015-10-14   \n",
              "9  10002A    2015-11-23  Enterprise    12.25  2015-10-14   \n",
              "\n",
              "             first_week first_month  \n",
              "0 2015-10-12/2015-10-18     2015-10  \n",
              "1 2015-10-12/2015-10-18     2015-10  \n",
              "2 2015-10-12/2015-10-18     2015-10  \n",
              "3 2015-10-12/2015-10-18     2015-10  \n",
              "4 2015-10-12/2015-10-18     2015-10  \n",
              "5 2015-10-12/2015-10-18     2015-10  \n",
              "6 2015-10-12/2015-10-18     2015-10  \n",
              "7 2015-10-12/2015-10-18     2015-10  \n",
              "8 2015-10-12/2015-10-18     2015-10  \n",
              "9 2015-10-12/2015-10-18     2015-10  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "JyYfAz4RxU6O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Combining the basic DAU data with the first date, week, and month for each user, **the DAU Decorated dataframe is our basic building block for many different analyses**. It allows us to use user-level data to inspect engagement, retention, and growth accounting."
      ]
    },
    {
      "metadata": {
        "id": "E9sHP5yDwAwH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.4 Calculate cohort analysis metrics\n",
        "Now that we have the \"DAU Decorated\" data frame, we can use it to calculate cohort retention and LTV metrics since they are super-important for an early-stage startup. The **create_xau_decorated_df** function below creates a monthly roll-up of the DAU Decorated data we call MAU Decorated."
      ]
    },
    {
      "metadata": {
        "id": "lZ2gzkY70psc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "outputId": "9adbb6df-ba6a-4183-8496-18c6fde392a8"
      },
      "cell_type": "code",
      "source": [
        "### For discrete time period calculations, this helps set the variable names\n",
        "### in the different dataframes \n",
        "def get_time_period_dict(time_period):\n",
        "    \n",
        "    time_fields_dict = {\n",
        "                        'day' : {'grouping_col' : 'activity_date',\n",
        "                                  'first_period_col' : 'first_dt',\n",
        "                                  'frequency' : 'Daily',\n",
        "                                  'unit' : 'Day',\n",
        "                                  'period_abbr' : 'D',\n",
        "                                  'python_period' : 'days',\n",
        "                                  'days' : 1\n",
        "                                  },\n",
        "                        'week' : {'grouping_col' : 'Week',\n",
        "                                  'first_period_col' : 'first_week',\n",
        "                                  'frequency' : 'Weekly',\n",
        "                                  'unit' : 'Week',\n",
        "                                  'period_abbr' : 'W',\n",
        "                                  'python_period' : 'weeks',\n",
        "                                  'days' : 7\n",
        "                                  },\n",
        "                        'month' : {'grouping_col' : 'Month_Year',\n",
        "                                   'first_period_col' : 'first_month',\n",
        "                                   'frequency' : 'Monthly',\n",
        "                                   'unit' : 'Month',\n",
        "                                   'period_abbr' : 'M',\n",
        "                                   'python_period' : 'months',\n",
        "                                   'days' : 28\n",
        "                                  }\n",
        "                        }\n",
        "                    \n",
        "    # if time_period passed in is a valid choice, then return the dictionary\n",
        "    # associated with that choice from the dictionary above\n",
        "    if time_period in time_fields_dict:\n",
        "        time_fields = time_fields_dict[time_period]\n",
        "    else:\n",
        "        time_fields = None\n",
        "    \n",
        "    return time_fields\n",
        "  \n",
        "  \n",
        "  \n",
        "### This is another helper function that allows us to determine the next week\n",
        "### or month for any given week of month. We need these because the Pandas\n",
        "### date math is not consistent. You can use timedelta to add weeks, but you \n",
        "### have to use DateOffset to add months.\n",
        "def increment_period(xau_grouping_col, time_period):\n",
        "  \n",
        "    # Call the get_time_period_dict function above, passing in the time period\n",
        "    time_fields = get_time_period_dict(time_period)\n",
        "    \n",
        "    # Set the one-letter period abbreviation to whatever that function returns\n",
        "    period_abbr = time_fields['period_abbr']\n",
        "    \n",
        "    # Depending on the time period, increment the week or month by one after\n",
        "    # first converting it to the date time of the start of the period\n",
        "    if time_period == 'week':\n",
        "        start_of_next_period = pd.to_datetime(pd.PeriodIndex(xau_grouping_col).start_time + timedelta(weeks = 1))\n",
        "    elif time_period == 'month':\n",
        "        start_of_next_period = pd.to_datetime(pd.PeriodIndex(xau_grouping_col, freq = period_abbr).start_time) + pd.DateOffset(months = 1)\n",
        "    else:\n",
        "        start_of_next_period = None\n",
        "        \n",
        "    # Convert the date time from the previous step back into the appropriate \n",
        "    # period (week or month) \n",
        "    if start_of_next_period is not None:\n",
        "        next_period = pd.Series(start_of_next_period).dt.to_period(period_abbr)\n",
        "    else:\n",
        "        next_period = None\n",
        "    \n",
        "    # Return the next period\n",
        "    return next_period\n",
        "  \n",
        "  \n",
        "### create_xau_decorated_df is a generic function that allows us to find WAU\n",
        "### Decorated or MAU Decorated from a DAU Decorated based on the time period\n",
        "### that gets passed in\n",
        "def create_xau_decorated_df(dau_decorated_df, time_period, use_segment):\n",
        "    \n",
        "    # These are the parameters that are set from the get_time_period_dict \n",
        "    # function above\n",
        "    time_fields = get_time_period_dict(time_period)\n",
        "    grouping_col = time_fields['grouping_col']\n",
        "    frequency = time_fields['frequency']\n",
        "    first_period_col = time_fields['first_period_col']\n",
        "    period_abbr = time_fields['period_abbr']\n",
        "    \n",
        "    # Print a notification message indicating that this function has been called\n",
        "    print('Creating ' + frequency + ' Active Users Decorated dataframe')\n",
        "    \n",
        "    # We are grouping by the grouping_col (which is either \"Week\" or \"Month_Year\"),\n",
        "    # the user_id, and the first_period_col (either \"first_week\" or \"first_month\")\n",
        "    # For each user_id, there is one and only one first_period_col.\n",
        "    \n",
        "    groupby_cols = [grouping_col, 'user_id', first_period_col]\n",
        "    \n",
        "    # If we are using the segment column, we include that in the groupby_cols\n",
        "    # list as well\n",
        "    if use_segment:\n",
        "        groupby_cols = groupby_cols + ['segment']\n",
        "        \n",
        "    # Start by making a copy of the dataframe that gets passed in so as not to\n",
        "    # affect the original\n",
        "    dau_decorated = dau_decorated_df.copy()\n",
        "    \n",
        "    # Convert the activity_date for each transaction in dau_decorated to a \n",
        "    # period the same timeframe as the period in question (either a week or a \n",
        "    # month)\n",
        "    dau_decorated[grouping_col] = pd.to_datetime(dau_decorated['activity_date']).dt.to_period(period_abbr)\n",
        "    \n",
        "    # Group dau_decorated into the grouping_cols defined above and aggregate the\n",
        "    # sum of the inc_amt field\n",
        "    xau = (dau_decorated.groupby(groupby_cols, as_index = False)['inc_amt'].sum())\n",
        "    \n",
        "    # Set a new column with the next time period by calling the increment_period\n",
        "    # function defined above\n",
        "    xau['Next_' + grouping_col] = increment_period(xau[grouping_col], time_period)\n",
        "    \n",
        "    # Select a subset of the resultant columns from the groupby to output\n",
        "    output_cols = [grouping_col, 'user_id', 'inc_amt', first_period_col, 'Next_' + grouping_col]\n",
        "    if use_segment:\n",
        "        output_cols = output_cols + ['segment']\n",
        "    xau = xau[output_cols]\n",
        "    \n",
        "    # Return the resultant dataset\n",
        "    return xau\n",
        "  \n",
        "  \n",
        "# Run create_xau_decorated_df for MAU's (using 'month'), unsegmented\n",
        "mau_decorated = create_xau_decorated_df(dau_decorated, 'month', use_segment=False)\n",
        "mau_decorated.tail(10)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating Monthly Active Users Decorated dataframe\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Month_Year</th>\n",
              "      <th>user_id</th>\n",
              "      <th>inc_amt</th>\n",
              "      <th>first_month</th>\n",
              "      <th>Next_Month_Year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>93658</th>\n",
              "      <td>2019-02</td>\n",
              "      <td>9784A</td>\n",
              "      <td>533.66</td>\n",
              "      <td>2015-09</td>\n",
              "      <td>2019-03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93659</th>\n",
              "      <td>2019-02</td>\n",
              "      <td>9794A</td>\n",
              "      <td>24.72</td>\n",
              "      <td>2015-09</td>\n",
              "      <td>2019-03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93660</th>\n",
              "      <td>2019-02</td>\n",
              "      <td>9808A</td>\n",
              "      <td>85.12</td>\n",
              "      <td>2015-09</td>\n",
              "      <td>2019-03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93661</th>\n",
              "      <td>2019-02</td>\n",
              "      <td>9868A</td>\n",
              "      <td>65.75</td>\n",
              "      <td>2015-10</td>\n",
              "      <td>2019-03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93662</th>\n",
              "      <td>2019-02</td>\n",
              "      <td>9876A</td>\n",
              "      <td>19.48</td>\n",
              "      <td>2015-10</td>\n",
              "      <td>2019-03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93663</th>\n",
              "      <td>2019-02</td>\n",
              "      <td>9902A</td>\n",
              "      <td>39.76</td>\n",
              "      <td>2015-10</td>\n",
              "      <td>2019-03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93664</th>\n",
              "      <td>2019-02</td>\n",
              "      <td>9952A</td>\n",
              "      <td>53.50</td>\n",
              "      <td>2015-10</td>\n",
              "      <td>2019-03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93665</th>\n",
              "      <td>2019-02</td>\n",
              "      <td>9986A</td>\n",
              "      <td>116.64</td>\n",
              "      <td>2015-10</td>\n",
              "      <td>2019-03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93666</th>\n",
              "      <td>2019-02</td>\n",
              "      <td>9989A</td>\n",
              "      <td>21.28</td>\n",
              "      <td>2015-10</td>\n",
              "      <td>2019-03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93667</th>\n",
              "      <td>2019-02</td>\n",
              "      <td>9995A</td>\n",
              "      <td>85.12</td>\n",
              "      <td>2015-10</td>\n",
              "      <td>2019-03</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Month_Year user_id  inc_amt first_month Next_Month_Year\n",
              "93658    2019-02   9784A   533.66     2015-09         2019-03\n",
              "93659    2019-02   9794A    24.72     2015-09         2019-03\n",
              "93660    2019-02   9808A    85.12     2015-09         2019-03\n",
              "93661    2019-02   9868A    65.75     2015-10         2019-03\n",
              "93662    2019-02   9876A    19.48     2015-10         2019-03\n",
              "93663    2019-02   9902A    39.76     2015-10         2019-03\n",
              "93664    2019-02   9952A    53.50     2015-10         2019-03\n",
              "93665    2019-02   9986A   116.64     2015-10         2019-03\n",
              "93666    2019-02   9989A    21.28     2015-10         2019-03\n",
              "93667    2019-02   9995A    85.12     2015-10         2019-03"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "q-hrJq9G27GZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The **create_xau_cohort_df** function below helps us calculate what happens to the cohort of users that comes in as a new customer each month. It looks at how many of them continue to use the product (expressed as a number and as a percentage), how much income they generate each month, the income per customer, and the cumulative income per customer. This information leads to several insightful visuals as we will see below."
      ]
    },
    {
      "metadata": {
        "id": "ewa5yayk3Cg4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "8dfcc531-d306-429a-c393-700dd883facb"
      },
      "cell_type": "code",
      "source": [
        "### Calculate the user retention and empirical CLTV using inc_amtby cohort \n",
        "### defined by any weekly or monthly time period\n",
        "def create_xau_cohort_df(xau_decorated_df, \n",
        "                         time_period, \n",
        "                         use_segment = False,\n",
        "                         recent_periods_back_to_exclude = 1, \n",
        "                         date_limit = None,\n",
        "                         create_period_n_inc_cols = False,\n",
        "                         add_hours = False,\n",
        "                         use_standard_col_names = False):\n",
        "    \n",
        "    # These are the parameters that are set from the get_time_period_dict \n",
        "    # function above    \n",
        "    time_fields = get_time_period_dict(time_period)\n",
        "    grouping_col = time_fields['grouping_col']\n",
        "    first_period_col = time_fields['first_period_col']\n",
        "    unit = time_fields['unit']\n",
        "    period_abbr = time_fields['period_abbr']\n",
        "        \n",
        "    # If a date_limit is set, set that date as the max date in the \n",
        "    # dau_decorated_df dataframe passed into the function.\n",
        "    # Otherwise, make a copy of the entire input dau_decorated_df.\n",
        "    if date_limit is not None:\n",
        "        xau_d = (xau_decorated_df[pd.PeriodIndex(xau_decorated_df[grouping_col], \n",
        "                                                 freq = period_abbr)\n",
        "                                  .start_time <= date_limit]\n",
        "                 .copy()\n",
        "                )\n",
        "    else:\n",
        "        xau_d = xau_decorated_df.copy()\n",
        "    \n",
        "    # Set the since_col variable to say \"Months Since First\" or \"Weeks Since First\"\n",
        "    since_col = '%ss Since First' % unit\n",
        "    \n",
        "    # Calculate the value in the since_col to be the number of periods between\n",
        "    # the current period and the user's first period\n",
        "    xau_d[since_col] = xau_d[grouping_col] - xau_d[first_period_col]\n",
        "    \n",
        "    # Since we are aggregating it all by the cohort of users that started in a\n",
        "    # particular period, we set the group by columns for the first aggregation\n",
        "    # as the first_period_col, grouping_col, and since_col\n",
        "    # first_period_col = {'first_month' | 'first_week' | 'first_day'}\n",
        "    # grouping_col = {'Month_Year' | 'Week' | 'activity_date'}\n",
        "    # since_col = {'Months Since First' | 'Weeks Since First' | 'Days Since First'}\n",
        "    first_groupby_cols = [first_period_col, grouping_col, since_col]\n",
        "    \n",
        "    # If we are including the segment in the calculations, include it in the \n",
        "    # groupby columns\n",
        "    if use_segment:\n",
        "        first_groupby_cols = first_groupby_cols + ['segment']\n",
        "    \n",
        "    # Group xau_d by the first_groupby_cols to find the sum of inc_amt and\n",
        "    # the number of unique user_ids in each grouping\n",
        "    xau_d = xau_d.groupby(first_groupby_cols)\\\n",
        "                    .agg({'inc_amt' : 'sum', \n",
        "                          'user_id' : 'nunique'})\\\n",
        "                    .rename(columns = { 'user_id' : 'cust_ct' })\n",
        "                    \n",
        "    # For the second groupby, we reduce the columns down to the first_period_col\n",
        "    # and the segment (if applicable)\n",
        "    second_groupby_cols = [first_period_col]\n",
        "    if use_segment:\n",
        "        second_groupby_cols = second_groupby_cols + ['segment']\n",
        "    \n",
        "    # The second groupby is used to make calculations about the cohort as a whole\n",
        "    # rather than at the individual period level\n",
        "    # The first of such calculations is to take the first value for cust_ct\n",
        "    # (customer count) as being the number of customers in the cohort\n",
        "    xau_d['cohort_cust_ct'] = xau_d.groupby(second_groupby_cols)['cust_ct'].transform('first')\n",
        "    \n",
        "    # The second calculation at the cohort level is to get the cumulative sum\n",
        "    # of the inc_amt for each period's cohort\n",
        "    xau_d['cum_inc_amt'] = xau_d.groupby(second_groupby_cols)['inc_amt'].cumsum()\n",
        "    \n",
        "    # These ratios are calculated per period using the per-cohort numbers calculated\n",
        "    # using the second groupby\n",
        "    # First we calculate cum_inc_per_cohort_cust (cumulative income per cohort\n",
        "    # customer) as that period's cumulative inc_amt divided by the total number \n",
        "    # of customers in the first period of the cohort\n",
        "    xau_d['cum_inc_per_cohort_cust'] = xau_d['cum_inc_amt'] / xau_d['cohort_cust_ct']\n",
        "    \n",
        "    # We also calculate how many users from the original count of cohort customers\n",
        "    # is still active in the current period. This is the cust_ret_pct (customer\n",
        "    # retention percentage)\n",
        "    xau_d['cust_ret_pct'] = xau_d['cust_ct'] / xau_d['cohort_cust_ct']\n",
        "    \n",
        "    # Reset the index on the Pandas dataframe\n",
        "    xau_d = xau_d.reset_index()\n",
        "    \n",
        "    # The code below removes rows, adds columns, or tweaks some of the time\n",
        "    # columns to allow for presentation in certain cases.\n",
        "    \n",
        "    # If the time period in question is 'day', then the logic works somewhat\n",
        "    # differently than it does if it is a 'week' or 'month'. Week/Month are in \n",
        "    # the top \"if\" clause, while Day is in the \"else\" clause.\n",
        "    if time_period != 'day':\n",
        "      \n",
        "        # If the time period is a month, and we have recent periods back to \n",
        "        # exclude (an input parameter), we use this code to exclude those \n",
        "        # periods from xau_d. The periods back is measured from TODAY's date.\n",
        "        if time_period == 'month':\n",
        "            td = pd.DateOffset(months = recent_periods_back_to_exclude)\n",
        "        elif time_period == 'week':\n",
        "            td = timedelta(weeks = recent_periods_back_to_exclude)\n",
        "        \n",
        "        last_period = pd.to_datetime(datetime.today() - td).to_period(period_abbr)\n",
        "        xau_d = xau_d.loc[xau_d[grouping_col] <= last_period]\n",
        "        \n",
        "        # If we want to add seven hours to the first_period_col and grouping_col\n",
        "        # we do so here if the add_hours input parameter is True. It defaults\n",
        "        # to False\n",
        "        if add_hours:\n",
        "            xau_d[first_period_col] = (pd.PeriodIndex(xau_d[first_period_col], \n",
        "                                                      freq = period_abbr)\n",
        "                                       .start_time + timedelta(hours = 7))\n",
        "            xau_d[grouping_col] = (pd.PeriodIndex(xau_d[grouping_col], \n",
        "                                                  freq = period_abbr)\n",
        "                                   .start_time + timedelta(hours = 7))\n",
        "        \n",
        "        # Using the segment column requires us to specify which segment each\n",
        "        # first_period_col goes with. \n",
        "        # Creator's note: needs string to handle the weekly case as well as the monthly\n",
        "        if use_segment:\n",
        "            xau_d['segment_first_' + time_period] = (xau_d[first_period_col]\n",
        "                                                     .dt\n",
        "                                                     .strftime('%Y-%m') + '-' + xau_d['segment'])  \n",
        "        \n",
        "        # If we want to add new columns for weekly trend analysis, we would do\n",
        "        # so by setting the create_period_n_inc_cols equal to True, which would\n",
        "        # trigger the add_period_n_cum_inc_per_cohort_cust_columns function\n",
        "        # to run. The parameter defaults to False.\n",
        "        if create_period_n_inc_cols:\n",
        "            xau_d = add_period_n_cum_inc_per_cohort_cust_columns(xau_d, since_col, unit)\n",
        "\n",
        "    else:\n",
        "        \n",
        "        # In the case of days, excluding the last period involves using \n",
        "        # timedelta to subtract days from today's date\n",
        "        last_period = (datetime.today() - timedelta(days = recent_periods_back_to_exclude)).date()\n",
        "        xau_d = xau_d.loc[xau_d[grouping_col] <= last_period]\n",
        "        \n",
        "        # The add_hours piece uses timedelta(hours = 7)\n",
        "        if add_hours:\n",
        "            xau_d[first_period_col] = xau_d[first_period_col] + timedelta(hours = 7)\n",
        "            xau_d[grouping_col] = xau_d[grouping_col] + timedelta(hours = 7)\n",
        "            \n",
        "        # Change the data type on the since_col to be a number and not a timedelta    \n",
        "        xau_d[since_col] = xau_d[since_col].astype(timedelta).map(lambda x: np.nan if pd.isnull(x) else x.days)\n",
        "        \n",
        "        # Using the segment column requires us to specify which segment each\n",
        "        # first_period_col goes with. \n",
        "        if use_segment:\n",
        "            xau_d['segment_first_' + time_period] = (xau_d[first_period_col]\n",
        "                                                     .dt\n",
        "                                                     .strftime('%Y-%m-%d') + '-' + xau_d['segment'])\n",
        "        \n",
        "        # Similar to above for adding weekly trend analysis, we would do\n",
        "        # so by setting the create_period_n_inc_cols equal to True, which would\n",
        "        # trigger the add_period_n_cum_inc_per_cohort_cust_columns function\n",
        "        # to run. The parameter defaults to False.\n",
        "        if create_period_n_inc_cols:\n",
        "            xau_d = add_period_n_cum_inc_per_cohort_cust_columns(xau_d, since_col, unit)\n",
        "            \n",
        "    # This last bit of code changes column names if the input parameter\n",
        "    # use_standard_col_names is True. It defaults to False\n",
        "    if use_standard_col_names:\n",
        "        mapping = {'month_year' : 'month',\n",
        "                   'cust_ret_pct' : 'retained_pctg',\n",
        "                   since_col : time_period + 's_since_first'\n",
        "                   }\n",
        "        \n",
        "        cols = xau_d.columns\n",
        "        new_xau_cols = []\n",
        "        for c in cols:\n",
        "            new_c = '_'.join(c.lower().split(' '))\n",
        "            if new_c in mapping:\n",
        "                new_c = mapping[new_c]\n",
        "            new_xau_cols.append(new_c)\n",
        "        print(new_xau_cols)\n",
        "        xau_d.columns = new_xau_cols\n",
        "\n",
        "    \n",
        "    return xau_d\n",
        "\n",
        "  \n",
        "mau_retention_by_cohort = create_xau_cohort_df(mau_decorated, 'month')\n",
        "mau_retention_by_cohort.tail(10)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>first_month</th>\n",
              "      <th>Month_Year</th>\n",
              "      <th>Months Since First</th>\n",
              "      <th>inc_amt</th>\n",
              "      <th>cust_ct</th>\n",
              "      <th>cohort_cust_ct</th>\n",
              "      <th>cum_inc_amt</th>\n",
              "      <th>cum_inc_per_cohort_cust</th>\n",
              "      <th>cust_ret_pct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1071</th>\n",
              "      <td>2018-11</td>\n",
              "      <td>2018-11</td>\n",
              "      <td>0</td>\n",
              "      <td>17178.12</td>\n",
              "      <td>518</td>\n",
              "      <td>518</td>\n",
              "      <td>17178.12</td>\n",
              "      <td>33.162394</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1072</th>\n",
              "      <td>2018-11</td>\n",
              "      <td>2018-12</td>\n",
              "      <td>1</td>\n",
              "      <td>20136.67</td>\n",
              "      <td>356</td>\n",
              "      <td>518</td>\n",
              "      <td>37314.79</td>\n",
              "      <td>72.036274</td>\n",
              "      <td>0.687259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1073</th>\n",
              "      <td>2018-11</td>\n",
              "      <td>2019-01</td>\n",
              "      <td>2</td>\n",
              "      <td>18325.46</td>\n",
              "      <td>300</td>\n",
              "      <td>518</td>\n",
              "      <td>55640.25</td>\n",
              "      <td>107.413610</td>\n",
              "      <td>0.579151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1074</th>\n",
              "      <td>2018-11</td>\n",
              "      <td>2019-02</td>\n",
              "      <td>3</td>\n",
              "      <td>14171.36</td>\n",
              "      <td>263</td>\n",
              "      <td>518</td>\n",
              "      <td>69811.61</td>\n",
              "      <td>134.771448</td>\n",
              "      <td>0.507722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1075</th>\n",
              "      <td>2018-12</td>\n",
              "      <td>2018-12</td>\n",
              "      <td>0</td>\n",
              "      <td>26134.01</td>\n",
              "      <td>740</td>\n",
              "      <td>740</td>\n",
              "      <td>26134.01</td>\n",
              "      <td>35.316230</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1076</th>\n",
              "      <td>2018-12</td>\n",
              "      <td>2019-01</td>\n",
              "      <td>1</td>\n",
              "      <td>31004.43</td>\n",
              "      <td>497</td>\n",
              "      <td>740</td>\n",
              "      <td>57138.44</td>\n",
              "      <td>77.214108</td>\n",
              "      <td>0.671622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1077</th>\n",
              "      <td>2018-12</td>\n",
              "      <td>2019-02</td>\n",
              "      <td>2</td>\n",
              "      <td>22879.87</td>\n",
              "      <td>379</td>\n",
              "      <td>740</td>\n",
              "      <td>80018.31</td>\n",
              "      <td>108.132851</td>\n",
              "      <td>0.512162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1078</th>\n",
              "      <td>2019-01</td>\n",
              "      <td>2019-01</td>\n",
              "      <td>0</td>\n",
              "      <td>20557.47</td>\n",
              "      <td>583</td>\n",
              "      <td>583</td>\n",
              "      <td>20557.47</td>\n",
              "      <td>35.261527</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1079</th>\n",
              "      <td>2019-01</td>\n",
              "      <td>2019-02</td>\n",
              "      <td>1</td>\n",
              "      <td>21454.77</td>\n",
              "      <td>388</td>\n",
              "      <td>583</td>\n",
              "      <td>42012.24</td>\n",
              "      <td>72.062161</td>\n",
              "      <td>0.665523</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1080</th>\n",
              "      <td>2019-02</td>\n",
              "      <td>2019-02</td>\n",
              "      <td>0</td>\n",
              "      <td>17109.64</td>\n",
              "      <td>537</td>\n",
              "      <td>537</td>\n",
              "      <td>17109.64</td>\n",
              "      <td>31.861527</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     first_month Month_Year  Months Since First   inc_amt  cust_ct  \\\n",
              "1071     2018-11    2018-11                   0  17178.12      518   \n",
              "1072     2018-11    2018-12                   1  20136.67      356   \n",
              "1073     2018-11    2019-01                   2  18325.46      300   \n",
              "1074     2018-11    2019-02                   3  14171.36      263   \n",
              "1075     2018-12    2018-12                   0  26134.01      740   \n",
              "1076     2018-12    2019-01                   1  31004.43      497   \n",
              "1077     2018-12    2019-02                   2  22879.87      379   \n",
              "1078     2019-01    2019-01                   0  20557.47      583   \n",
              "1079     2019-01    2019-02                   1  21454.77      388   \n",
              "1080     2019-02    2019-02                   0  17109.64      537   \n",
              "\n",
              "      cohort_cust_ct  cum_inc_amt  cum_inc_per_cohort_cust  cust_ret_pct  \n",
              "1071             518     17178.12                33.162394      1.000000  \n",
              "1072             518     37314.79                72.036274      0.687259  \n",
              "1073             518     55640.25               107.413610      0.579151  \n",
              "1074             518     69811.61               134.771448      0.507722  \n",
              "1075             740     26134.01                35.316230      1.000000  \n",
              "1076             740     57138.44                77.214108      0.671622  \n",
              "1077             740     80018.31               108.132851      0.512162  \n",
              "1078             583     20557.47                35.261527      1.000000  \n",
              "1079             583     42012.24                72.062161      0.665523  \n",
              "1080             537     17109.64                31.861527      1.000000  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "hzG-fUwkyM9K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3. Load the transformed data into Google Sheets \n",
        "### Establish connection to Google Sheets for writing output files\n",
        "The first time you run this cell, or after some time of inactivity, you will be asked to click on a link. That link will take you to a new tab that will authorize this script to write to Google Sheets spreadsheets in your Google Account. To enable this feature, copy the code you get into the box below and hit Enter."
      ]
    },
    {
      "metadata": {
        "id": "6XdHduQeyNSF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "gc = gspread.authorize(GoogleCredentials.get_application_default())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hKW2TNyWyf-1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Function to facilitate writing from Pandas to Google Sheets\n",
        "For use further down in this notebook."
      ]
    },
    {
      "metadata": {
        "id": "cKXbV7w0ygK4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def write_to_google_sheet(dataframe, spreadsheet_key, worksheet_name, goog_creds = gc):\n",
        "  \n",
        "  sh = goog_creds.open_by_key(spreadsheet_key)\n",
        "  \n",
        "  ws = None\n",
        "  worksheet_list = sh.worksheets()\n",
        "  for worksheet in worksheet_list:\n",
        "    if worksheet.title == worksheet_name:\n",
        "      ws = worksheet\n",
        "      \n",
        "  if ws is None:\n",
        "    ws = sh.add_worksheet(title = worksheet_name, rows=\"1\", cols = \"1\")\n",
        "    \n",
        "  set_with_dataframe(ws, dataframe, row=1, col=1, include_index=False, \n",
        "                     include_column_header=True, resize=True, allow_formulas=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "by-pAUIe3St7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Create Google Sheet for writing output files if one is not already established\n",
        "\n",
        "***Warning: you must choose from Options A and B below***.\n",
        "\n",
        "**Option A**: If you already have a Google Sheet where you store transformed data for analytics, go to that sheet, copy the long ID string from the sheet's URL in a browser, and paste it between the quotes below. Then uncomment the code and run the cell. Do NOT run the code for Option B without commenting it out.\n",
        "\n",
        "This option is especially important if you have previously linked a reporting dashboard to this Google Sheet and would like to use this ETL process to update that dashboard."
      ]
    },
    {
      "metadata": {
        "id": "GwwRwkfizC7Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### Option A Code\n",
        "GOOGLE_SHEET_KEY = '1-XnO_eWkRwX-E1fiA2Jkbe3kJvoyoPFsdeW7vnF6zS0' "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jNR7YkOY6jQX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Option B**: If you are running this for the first time and do not already have a Google Sheet where you store transformed data for analytics, uncomment and run the cell below to set the GOOGLE_SHEET_KEY variable, which will be used later in the process. Be sure to replace the \"Sample Company Analytics\" filename with one of your own."
      ]
    },
    {
      "metadata": {
        "id": "zSzoG5Bn3h8N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### Option B Code\n",
        "# GOOGLE_SHEET_KEY = gc.create('Sample Company Analytics').id"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CUX6Br7s7ibA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "After executing the cell above, a new spreadsheet with the name you supply will be shown in your sheets list on [sheets.google.com](http://sheets.google.com/). To go directly to the newly created Google Sheet, run the code block below and visit the link that it outputs. You can keep this Sheet open in a separate tab to see the data get updated whenever **write_to_google_sheet** (defined above) is called."
      ]
    },
    {
      "metadata": {
        "id": "6nn3Co-B9osp",
        "colab_type": "code",
        "outputId": "d1fac740-6445-49a8-bdcf-b52c5fab7f88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print('https://docs.google.com/spreadsheets/d/' + GOOGLE_SHEET_KEY)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://docs.google.com/spreadsheets/d/1-XnO_eWkRwX-E1fiA2Jkbe3kJvoyoPFsdeW7vnF6zS0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YTuA7GJxi8P5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "write_to_google_sheet(mau_retention_by_cohort, GOOGLE_SHEET_KEY, 'MAU Retention by Cohort')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XgoZiaUa8Txs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4. Visualize insights in Google Data Studio\n",
        "A Google DataStudio dashboard preconfigured to read from the Google Sheet created above to visualize the data is [available at this link](https://datastudio.google.com/open/1xjS__Q6ZUXuUUARkgRvY4spYUw1ePksV) or by clicking on the Google DataStudio logo at the bottom of the chart embedded below. It is available in read-only mode for you to copy, link to your own Google Sheet tabs, and see your own data visualized. \n",
        "\n",
        "Note: sometimes there is an issue embedding the Google DataStudio dashboard in this notebook, particularly when it is rendered in the GitHub notebook reader. In that case, click on the \"Open in Colab\" button at the very top of the notebook to see a notebook that can embed the dashboard properly.\n",
        "### 4.1 Monthly Cohort User Retention\n",
        "One way to think about cohort user retention is based on the fact that, typically, not all users are retained from Month 0 to Month 1, and then to Month 2, 3, 4, etc. The chart below depicts this in the downward curve approaching. If you mouseover the bars, we can see that the January 2018 cohort at the farthest right is at 23.7% user retention in Month 13. Compare that to the 60.94% figure for that same cohort in Month 1.\n",
        "\n",
        "Sometimes this data is depicted as downward curving lines, one for each cohort. We prefer this visual because (a) the lines often end up looking jumbled and spaghetti-like; and (b) it allows us to see the trend in each Months Since First's retention. For example, the most recent Month 1 retention of 66.55% for the January 2019 cohort is down from the Month 1 peak of 72.8% percent for the July 2018 cohort. In this way, we can see the general downward slope that the spaghetti lines would give us, but also see the trends within each Months Since First, for a superior visual."
      ]
    },
    {
      "metadata": {
        "id": "Jtw28m7Dlwo-",
        "colab_type": "code",
        "outputId": "c9ff5bd4-adab-417d-fbdb-eccfdd562370",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        }
      },
      "cell_type": "code",
      "source": [
        "IFrame('https://datastudio.google.com/embed/reporting/1xjS__Q6ZUXuUUARkgRvY4spYUw1ePksV/page/EOyj', \n",
        "       width=600, \n",
        "       height=450)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"600\"\n",
              "            height=\"450\"\n",
              "            src=\"https://datastudio.google.com/embed/reporting/1xjS__Q6ZUXuUUARkgRvY4spYUw1ePksV/page/EOyj\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7f94fa9c2438>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "xookNSymJYcR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 4.2 Cumulative Customer Revenue LTV by Monthly Cohort\n",
        "The visual below shows the cumulative inc_amt *per original cohort customer* that each cohort has generated in the time it has been active. These lines give a look at how well a group of ServBiz's users keeps generating revenue after they are initially acquired. This gives us an empirical way to look at customer long-term value (LTV) over a range of possible scenarios. For example, if you mouseover 16 Months Since First in the chart below, the four cohorts that have been around that long range from `$`440.47 to `$`588.87 per cohort customer. We can use that as a 16-month LTV range for projection or for calculating LTV-to-CAC ratio.\n",
        "\n",
        "Another feature of the curves below is that, while they are slighly bending downward, the rate of change in the slope is very gradual. This implies that ServBiz's customers are active for a long period of time, meaning that the time horizon for calculating LTV may be extended and that these customers are quite value in the long-term."
      ]
    },
    {
      "metadata": {
        "id": "MEczC7003zvl",
        "colab_type": "code",
        "outputId": "a10208f6-7138-4b35-dfbd-105594c41673",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        }
      },
      "cell_type": "code",
      "source": [
        "IFrame('https://datastudio.google.com/embed/reporting/1xjS__Q6ZUXuUUARkgRvY4spYUw1ePksV/page/pVBk', \n",
        "       width=600, \n",
        "       height=450)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"600\"\n",
              "            height=\"450\"\n",
              "            src=\"https://datastudio.google.com/embed/reporting/1xjS__Q6ZUXuUUARkgRvY4spYUw1ePksV/page/pVBk\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7f94fa9c24a8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "azSCjmD5Rrsb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}